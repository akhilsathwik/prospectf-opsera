name: "prospectf500-app1 Deploy"

on:
  push:
    branches:
      - prospectf500-app1-opsera
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'prospectf500-app1-opsera/k8s/**'
      - '.github/workflows/prospectf500-app1-deploy.yaml'
  workflow_dispatch:

env:
  AWS_REGION: eu-north-1
  APP_IDENTIFIER: prospectf500-app1
  TENANT: opsera-se
  ENVIRONMENT: dev

permissions:
  contents: write

jobs:
  build-and-push:
    name: "Build and Push Images"
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.vars.outputs.image_tag }}
      aws_account_id: ${{ steps.vars.outputs.aws_account_id }}
      ecr_registry: ${{ steps.vars.outputs.ecr_registry }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up variables
        id: vars
        run: |
          IMAGE_TAG=$(echo ${{ github.sha }} | cut -c1-7)
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT
          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT
          
          echo "Image tag: $IMAGE_TAG"
          echo "AWS Account ID: $AWS_ACCOUNT_ID"
          echo "ECR Registry: $ECR_REGISTRY"

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repos if not exist
        run: |
          aws ecr describe-repositories --repository-names ${{ env.APP_IDENTIFIER }}-backend --region ${{ env.AWS_REGION }} 2>/dev/null || \
            aws ecr create-repository --repository-name ${{ env.APP_IDENTIFIER }}-backend --region ${{ env.AWS_REGION }}

          aws ecr describe-repositories --repository-names ${{ env.APP_IDENTIFIER }}-frontend --region ${{ env.AWS_REGION }} 2>/dev/null || \
            aws ecr create-repository --repository-name ${{ env.APP_IDENTIFIER }}-frontend --region ${{ env.AWS_REGION }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push backend
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: true
          tags: |
            ${{ steps.vars.outputs.ecr_registry }}/${{ env.APP_IDENTIFIER }}-backend:${{ steps.vars.outputs.image_tag }}
            ${{ steps.vars.outputs.ecr_registry }}/${{ env.APP_IDENTIFIER }}-backend:latest
          platforms: linux/amd64
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push frontend
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          push: true
          tags: |
            ${{ steps.vars.outputs.ecr_registry }}/${{ env.APP_IDENTIFIER }}-frontend:${{ steps.vars.outputs.image_tag }}
            ${{ steps.vars.outputs.ecr_registry }}/${{ env.APP_IDENTIFIER }}-frontend:latest
          platforms: linux/amd64
          cache-from: type=gha
          cache-to: type=gha,mode=max

  update-manifests:
    name: "Update K8s Manifests"
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Update kustomization with new image tags (Fix #82: Update ALL images)
        run: |
          cd prospectf500-app1-opsera/k8s/overlays/${{ env.ENVIRONMENT }}

          # Get AWS Account ID
          AWS_ACCOUNT_ID="${{ needs.build-and-push.outputs.aws_account_id }}"
          ECR_REGISTRY="${{ needs.build-and-push.outputs.ecr_registry }}"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image_tag }}"
          APP_IDENTIFIER="${{ env.APP_IDENTIFIER }}"

          # Replace ALL instances of ACCOUNT_ID placeholder (Fix #81)
          sed -i "s|ACCOUNT_ID|${AWS_ACCOUNT_ID}|g" kustomization.yaml

          # Update backend image newName (Fix #82: Update ALL images)
          sed -i "s|newName: .*${APP_IDENTIFIER}-backend|newName: ${ECR_REGISTRY}/${APP_IDENTIFIER}-backend|" kustomization.yaml

          # Update frontend image newName (Fix #82: Update ALL images)
          sed -i "s|newName: .*${APP_IDENTIFIER}-frontend|newName: ${ECR_REGISTRY}/${APP_IDENTIFIER}-frontend|" kustomization.yaml

          # Update ALL tags
          sed -i "s|newTag: .*|newTag: ${IMAGE_TAG}|g" kustomization.yaml

          # Validate no placeholders remain (Fix #81)
          if grep -q "ACCOUNT_ID\|PLACEHOLDER\|TODO\|CHANGEME" kustomization.yaml; then
            echo "ERROR: Placeholders found in kustomization.yaml"
            cat kustomization.yaml
            exit 1
          fi

          echo "Updated kustomization.yaml:"
          cat kustomization.yaml

      - name: Commit and push changes (Fix #89: Handle conflicts)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add prospectf500-app1-opsera/k8s/overlays/${{ env.ENVIRONMENT }}/kustomization.yaml

          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git pull --rebase origin prospectf500-app1-opsera || true
            git commit -m "[skip ci] Update image tags to ${{ needs.build-and-push.outputs.image_tag }}"
            git push || echo "Push conflict - will retry on next run"
          fi

  deploy-to-cluster:
    name: "Deploy to Workload Cluster"
    runs-on: ubuntu-latest
    needs: [build-and-push, update-manifests]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: prospectf500-app1-opsera

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check if workload cluster exists
        id: check-cluster
        run: |
          # Use shortened cluster name: prospectf500-app1-wrk-dev
          if aws eks describe-cluster --name ${{ env.APP_IDENTIFIER }}-wrk-${{ env.ENVIRONMENT }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            echo "Workload cluster found"
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            echo "WARNING: Workload cluster not found. Run infrastructure workflow first."
          fi

      - name: Configure kubectl
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        run: |
          # Use shortened cluster name: prospectf500-app1-wrk-dev
          aws eks update-kubeconfig --name ${{ env.APP_IDENTIFIER }}-wrk-${{ env.ENVIRONMENT }} --region ${{ env.AWS_REGION }}
          
      - name: Verify ECR images exist
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        run: |
          echo "=== Verifying ECR images exist ==="
          AWS_ACCOUNT_ID="${{ needs.build-and-push.outputs.aws_account_id }}"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image_tag }}"
          
          echo "Checking backend image..."
          aws ecr describe-images --repository-name ${{ env.APP_IDENTIFIER }}-backend --image-ids imageTag=$IMAGE_TAG --region ${{ env.AWS_REGION }} || {
            echo "ERROR: Backend image with tag $IMAGE_TAG not found in ECR"
            echo "Available tags:"
            aws ecr list-images --repository-name ${{ env.APP_IDENTIFIER }}-backend --region ${{ env.AWS_REGION }} --query 'imageIds[*].imageTag' || true
            exit 1
          }
          
          echo "Checking frontend image..."
          aws ecr describe-images --repository-name ${{ env.APP_IDENTIFIER }}-frontend --image-ids imageTag=$IMAGE_TAG --region ${{ env.AWS_REGION }} || {
            echo "ERROR: Frontend image with tag $IMAGE_TAG not found in ECR"
            echo "Available tags:"
            aws ecr list-images --repository-name ${{ env.APP_IDENTIFIER }}-frontend --region ${{ env.AWS_REGION }} --query 'imageIds[*].imageTag' || true
            exit 1
          }
          
          echo "✓ Both images exist in ECR"

      - name: Deploy with Kustomize
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        run: |
          echo "Deploying to namespace: ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}"
          kubectl apply -k prospectf500-app1-opsera/k8s/overlays/${{ env.ENVIRONMENT }}
          
          echo ""
          echo "=== Deployment Applied ==="
          kubectl get deployments -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
          kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}

      - name: Check pod status and events
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        continue-on-error: true
        run: |
          echo "=== Checking pod status ==="
          kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o wide
          
          echo ""
          echo "=== Deployment status ==="
          kubectl get deployments -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
          
          echo ""
          echo "=== Recent events (sorted by time) ==="
          kubectl get events -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --sort-by='.lastTimestamp' | tail -30
          
          echo ""
          echo "=== Checking for failed pods ==="
          FAILED_PODS=$(kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --field-selector=status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$FAILED_PODS" ]; then
            echo "Found failed/pending pods: $FAILED_PODS"
            for POD in $FAILED_PODS; do
              echo ""
              echo "=== Pod: $POD ==="
              kubectl describe pod $POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} | grep -A 20 "Events:" || true
              kubectl describe pod $POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} | grep -A 10 "State:" || true
              
              # Check for image pull errors
              IMAGE_ERROR=$(kubectl describe pod $POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} | grep -i "image\|pull\|errImagePull\|ImagePullBackOff" || echo "")
              if [ -n "$IMAGE_ERROR" ]; then
                echo "⚠ IMAGE PULL ERROR DETECTED:"
                echo "$IMAGE_ERROR"
              fi
            done
          fi
          
          echo ""
          echo "=== Backend pod details ==="
          BACKEND_POD=$(kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -l app=${{ env.APP_IDENTIFIER }}-backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$BACKEND_POD" ]; then
            echo "Backend pod: $BACKEND_POD"
            echo "Pod status:"
            kubectl get pod $BACKEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.status}' | jq '.' || kubectl get pod $BACKEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "Pod describe (full):"
            kubectl describe pod $BACKEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "Pod logs (if container started):"
            kubectl logs $BACKEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --tail=50 2>&1 || echo "Could not get logs (container may not have started)"
          else
            echo "No backend pod found"
          fi
          
          echo ""
          echo "=== Frontend pod details ==="
          FRONTEND_POD=$(kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -l app=${{ env.APP_IDENTIFIER }}-frontend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$FRONTEND_POD" ]; then
            echo "Frontend pod: $FRONTEND_POD"
            echo "Pod status:"
            kubectl get pod $FRONTEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.status}' | jq '.' || kubectl get pod $FRONTEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "Pod describe (full):"
            kubectl describe pod $FRONTEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "Pod logs (if container started):"
            kubectl logs $FRONTEND_POD -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --tail=50 2>&1 || echo "Could not get logs (container may not have started)"
          else
            echo "No frontend pod found"
          fi
          
          echo ""
          echo "=== Checking actual deployed image names ==="
          kubectl get deployment ${{ env.APP_IDENTIFIER }}-backend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.spec.template.spec.containers[0].image}' && echo "" || true
          kubectl get deployment ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.spec.template.spec.containers[0].image}' && echo "" || true

      - name: Wait for deployments
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "=== Waiting for backend deployment ==="
          kubectl rollout status deployment/${{ env.APP_IDENTIFIER }}-backend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --timeout=300s || {
            echo ""
            echo "⚠ Backend deployment timeout - this is expected if pods are failing"
            echo "Check the 'Check pod status and events' step above for details"
            echo ""
            echo "Deployment status:"
            kubectl get deployment ${{ env.APP_IDENTIFIER }}-backend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "ReplicaSet status:"
            kubectl get rs -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -l app=${{ env.APP_IDENTIFIER }}-backend
            exit 1
          }
          
          echo "=== Waiting for frontend deployment ==="
          kubectl rollout status deployment/${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --timeout=300s || {
            echo ""
            echo "⚠ Frontend deployment timeout - this is expected if pods are failing"
            echo "Check the 'Check pod status and events' step above for details"
            echo ""
            echo "Deployment status:"
            kubectl get deployment ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "ReplicaSet status:"
            kubectl get rs -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -l app=${{ env.APP_IDENTIFIER }}-frontend
            exit 1
          }

      - name: Get deployment status
        if: steps.check-cluster.outputs.cluster_exists == 'true'
        run: |
          echo "=== Pods ==="
          kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}

          echo ""
          echo "=== Services ==="
          kubectl get svc -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}

          echo ""
          echo "=== LoadBalancer URL ==="
          LB_HOSTNAME=$(kubectl get svc ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$LB_HOSTNAME" ] && [ "$LB_HOSTNAME" != "" ]; then
            echo "✓ LoadBalancer ready!"
            echo ""
            echo "ELB URL: http://$LB_HOSTNAME"
            echo "ELB URL (HTTPS): https://$LB_HOSTNAME"
            echo ""
            echo "Note: HTTPS may not work until ACM certificate is configured"
            echo ""
            echo "To access the application:"
            echo "  curl http://$LB_HOSTNAME"
            echo ""
            echo "Or wait for DNS record:"
            echo "  https://prospectf500-app1-dev.agents.opsera-labs.com"
          else
            echo "⚠ LoadBalancer is still pending..."
            echo "This usually takes 2-5 minutes after service creation"
            echo ""
            echo "Check service status:"
            kubectl describe svc ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} | grep -A 10 "Events:" || true
          fi

  check-dns:
    name: "Check DNS and ExternalDNS Status"
    runs-on: ubuntu-latest
    needs: deploy-to-cluster
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --name ${{ env.APP_IDENTIFIER }}-wrk-${{ env.ENVIRONMENT }} --region ${{ env.AWS_REGION }}

      - name: Check Service and LoadBalancer
        run: |
          echo "=== Checking Service Status ==="
          kubectl get svc ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} || echo "Service not found"
          
          echo ""
          echo "=== LoadBalancer Endpoint ==="
          LB_HOSTNAME=$(kubectl get svc ${{ env.APP_IDENTIFIER }}-frontend -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "PENDING")
          if [ "$LB_HOSTNAME" != "PENDING" ] && [ -n "$LB_HOSTNAME" ]; then
            echo "✓ LoadBalancer created: $LB_HOSTNAME"
            
            # Extract LoadBalancer ARN/name to find target groups
            echo ""
            echo "=== Checking LoadBalancer Target Group Health ==="
            LB_NAME=$(echo $LB_HOSTNAME | cut -d'.' -f1)
            echo "LoadBalancer name: $LB_NAME"
            
            # Find target groups associated with this LoadBalancer
            TARGET_GROUPS=$(aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} --query "LoadBalancers[?contains(DNSName, '$LB_NAME')].LoadBalancerArn" --output text 2>/dev/null || echo "")
            
            if [ -n "$TARGET_GROUPS" ]; then
              LB_ARN=$(echo $TARGET_GROUPS | awk '{print $1}')
              echo "LoadBalancer ARN: $LB_ARN"
              
              # Get target groups
              TG_ARNS=$(aws elbv2 describe-target-groups --load-balancer-arn $LB_ARN --region ${{ env.AWS_REGION }} --query 'TargetGroups[*].TargetGroupArn' --output text 2>/dev/null || echo "")
              
              if [ -n "$TG_ARNS" ]; then
                echo ""
                echo "Target Groups:"
                for TG_ARN in $TG_ARNS; do
                  echo "  - $TG_ARN"
                  
                  # Check target health
                  echo ""
                  echo "  Target Health:"
                  aws elbv2 describe-target-health --target-group-arn $TG_ARN --region ${{ env.AWS_REGION }} --query 'TargetHealthDescriptions[*].[Target.Id,TargetHealth.State,TargetHealth.Reason]' --output table 2>/dev/null || echo "    Could not get target health"
                done
              else
                echo "⚠ No target groups found for LoadBalancer"
              fi
            else
              echo "⚠ Could not find LoadBalancer ARN"
            fi
          else
            echo "⚠ LoadBalancer is still pending..."
          fi
          
          echo ""
          echo "=== CRITICAL: Checking if Pods are Running ==="
          RUNNING_PODS=$(kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }} --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          if [ -z "$RUNNING_PODS" ]; then
            echo "❌ NO PODS ARE RUNNING!"
            echo "This is why the LoadBalancer connection times out."
            echo ""
            echo "All pods:"
            kubectl get pods -n ${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}
            echo ""
            echo "Check the 'Check pod status and events' step in the deploy-to-cluster job for details."
          else
            echo "✓ Running pods: $RUNNING_PODS"
          fi

      - name: Check ExternalDNS Status
        run: |
          echo "=== ExternalDNS Pod Status ==="
          kubectl get pods -n kube-system -l app=external-dns
          
          echo ""
          echo "=== ExternalDNS Logs (last 50 lines) ==="
          EXTERNALDNS_POD=$(kubectl get pods -n kube-system -l app=external-dns -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$EXTERNALDNS_POD" ]; then
            kubectl logs $EXTERNALDNS_POD -n kube-system --tail=50 || echo "Could not get logs"
          else
            echo "ExternalDNS pod not found"
          fi

      - name: Check Route53 DNS Record
        run: |
          DOMAIN="prospectf500-app1-dev.agents.opsera-labs.com"
          HOSTED_ZONE="opsera-labs.com"
          
          echo "=== Checking Route53 DNS Record ==="
          echo "Domain: $DOMAIN"
          
          # Get hosted zone ID
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='${HOSTED_ZONE}.'].[Id]" --output text 2>/dev/null | sed 's|/hostedzone/||' || echo "")
          
          if [ -z "$HOSTED_ZONE_ID" ]; then
            echo "⚠ Route53 hosted zone '${HOSTED_ZONE}' not found"
            echo "Available hosted zones:"
            aws route53 list-hosted-zones --query 'HostedZones[*].Name' --output table || echo "Could not list hosted zones"
          else
            echo "✓ Hosted zone found: $HOSTED_ZONE_ID"
            
            # Check for DNS record
            DNS_RECORD=$(aws route53 list-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --query "ResourceRecordSets[?Name=='${DOMAIN}.']" --output json 2>/dev/null || echo "[]")
            
            if echo "$DNS_RECORD" | grep -q "$DOMAIN"; then
              echo "✓ DNS record EXISTS for $DOMAIN"
              aws route53 list-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --query "ResourceRecordSets[?Name=='${DOMAIN}.']" --output table
            else
              echo "⚠ DNS record NOT found for $DOMAIN"
              echo ""
              echo "This is normal if:"
              echo "  - LoadBalancer was just created (takes 2-5 minutes)"
              echo "  - ExternalDNS is still processing"
              echo ""
              echo "DNS propagation can take 5-10 minutes after ExternalDNS creates the record."
            fi
          fi

  verify-endpoint:
    name: "Verify Endpoint (Fix #83: Mandatory)"
    runs-on: ubuntu-latest
    needs: deploy-to-cluster
    timeout-minutes: 15

    steps:
      - name: Wait for endpoint (Fix #83: Mandatory verification)
        run: |
          ENDPOINT="https://${{ env.APP_IDENTIFIER }}-${{ env.ENVIRONMENT }}.agents.opsera-labs.com"
          echo "Checking endpoint: $ENDPOINT"

          for i in {1..30}; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "$ENDPOINT" 2>/dev/null || echo "000")
            echo "Attempt $i/30: HTTP Status = $HTTP_STATUS"

            if [ "$HTTP_STATUS" = "200" ] || [ "$HTTP_STATUS" = "301" ] || [ "$HTTP_STATUS" = "302" ]; then
              echo ""
              echo "=========================================="
              echo "DEPLOYMENT SUCCESSFUL"
              echo "Endpoint: $ENDPOINT"
              echo "=========================================="
              exit 0
            fi
            sleep 10
          done

          echo ""
          echo "WARNING: Endpoint not responding yet."
          echo "This may be normal for new deployments - DNS and LoadBalancer can take 5-10 minutes."
          echo "Check manually: $ENDPOINT"
