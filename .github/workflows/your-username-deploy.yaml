name: Deploy to AWS EKS

on:
  workflow_dispatch:
    inputs:
      tenant_name:
        description: 'Tenant/Organization name'
        required: true
        type: string
        default: 'opsera-se'
      app_name:
        description: 'Application name'
        required: true
        type: string
        default: 'your-username'
      app_env:
        description: 'Target environment'
        required: true
        type: choice
        options: [dev, staging, prod]
        default: 'dev'
      app_region:
        description: 'AWS deployment region'
        required: true
        type: string
        default: 'us-west-2'

permissions:
  contents: write
  id-token: write
  actions: read

# Short naming convention (Learning #118)
# Region short: us-west-2 -> usw2
# Tenant short: first 6 chars, lowercase
# Cluster env short: nonprod -> np, prod -> prod

env:
  # Derive deploy branch from app name
  DEPLOY_BRANCH: ${{ inputs.app_name }}-deploy
  # Short region code (derive from region input)
  REGION_SHORT: ${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}
  # Short cluster env (np = nonprod, prod = prod)
  CLUSTER_ENV_SHORT: ${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  # Resource names using short convention
  VPC_NAME: opsera-vpc
  ARGOCD_CLUSTER: argocd-${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}
  WORKLOAD_CLUSTER: ${{ inputs.tenant_name }}-${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}-${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  ECR_REPO: ${{ inputs.tenant_name }}/${{ inputs.app_name }}
  NAMESPACE: ${{ inputs.app_name }}-${{ inputs.app_env }}

jobs:
  infrastructure:
    name: "Phase 1: Infrastructure"
    runs-on: ubuntu-latest
    outputs:
      argocd_cluster: ${{ steps.discover.outputs.argocd_cluster }}
      workload_cluster: ${{ steps.discover.outputs.workload_cluster }}
      ecr_repo: ${{ steps.discover.outputs.ecr_repo }}
      namespace: ${{ steps.discover.outputs.namespace }}
      deployment_type: ${{ steps.discover.outputs.deployment_type }}
      aws_account_id: ${{ steps.discover.outputs.aws_account_id }}
      vpc_id: ${{ steps.discover.outputs.vpc_id }}
      public_subnet_ids: ${{ steps.discover.outputs.public_subnet_ids }}
      private_subnet_ids: ${{ steps.discover.outputs.private_subnet_ids }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Discover Infrastructure
        id: discover
        run: |
          echo "========================================"
          echo "INFRASTRUCTURE DISCOVERY"
          echo "========================================"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT

          ARGOCD_CLUSTER="${{ env.ARGOCD_CLUSTER }}"
          WORKLOAD_CLUSTER="${{ env.WORKLOAD_CLUSTER }}"
          ECR_REPO="${{ env.ECR_REPO }}"
          NAMESPACE="${{ env.NAMESPACE }}"

          echo "argocd_cluster=$ARGOCD_CLUSTER" >> $GITHUB_OUTPUT
          echo "workload_cluster=$WORKLOAD_CLUSTER" >> $GITHUB_OUTPUT
          echo "ecr_repo=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "namespace=$NAMESPACE" >> $GITHUB_OUTPUT

          # Check VPC (Learning #116)
          echo "Checking VPC: ${{ env.VPC_NAME }}..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ -n "$VPC_ID" ] && [ "$VPC_ID" != "null" ]; then
            VPC_EXISTS="true"
            echo "  VPC EXISTS: $VPC_ID"
          else
            VPC_EXISTS="false"
            echo "  VPC NOT FOUND - will create"
          fi
          echo "vpc_exists=$VPC_EXISTS" >> $GITHUB_OUTPUT
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

          # Check ArgoCD Cluster
          echo "Checking ArgoCD Cluster: $ARGOCD_CLUSTER..."
          if aws eks describe-cluster --name "$ARGOCD_CLUSTER" --region ${{ inputs.app_region }} 2>/dev/null; then
            ARGOCD_EXISTS="true"
            echo "  ArgoCD cluster EXISTS"
          else
            ARGOCD_EXISTS="false"
            echo "  ArgoCD cluster NOT FOUND - will create"
          fi
          echo "argocd_exists=$ARGOCD_EXISTS" >> $GITHUB_OUTPUT

          # Check Workload Cluster
          echo "Checking Workload Cluster: $WORKLOAD_CLUSTER..."
          if aws eks describe-cluster --name "$WORKLOAD_CLUSTER" --region ${{ inputs.app_region }} 2>/dev/null; then
            WORKLOAD_EXISTS="true"
            echo "  Workload cluster EXISTS"
          else
            WORKLOAD_EXISTS="false"
            echo "  Workload cluster NOT FOUND - will create"
          fi
          echo "workload_exists=$WORKLOAD_EXISTS" >> $GITHUB_OUTPUT

          # Check ECR Repository
          echo "Checking ECR Repository: $ECR_REPO..."
          if aws ecr describe-repositories --repository-names "$ECR_REPO" --region ${{ inputs.app_region }} 2>/dev/null; then
            ECR_EXISTS="true"
            echo "  ECR repository EXISTS"
          else
            ECR_EXISTS="false"
            echo "  ECR repository NOT FOUND - will create"
          fi
          echo "ecr_exists=$ECR_EXISTS" >> $GITHUB_OUTPUT

          # Determine deployment type
          if [ "$ARGOCD_EXISTS" = "true" ] && [ "$WORKLOAD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="brownfield"
          elif [ "$ARGOCD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="partial"
          else
            DEPLOY_TYPE="greenfield"
          fi
          echo "deployment_type=$DEPLOY_TYPE" >> $GITHUB_OUTPUT

          echo "========================================"
          echo "DEPLOYMENT TYPE: $DEPLOY_TYPE"
          echo "========================================"

      # Learning #114: Region-specific Terraform state bucket
      - name: Setup Terraform State Backend
        id: tf_backend
        run: |
          echo "Setting up Terraform State Backend..."

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          REGION="${{ inputs.app_region }}"

          # CRITICAL: Use region-specific bucket name (Learning #114)
          BUCKET_NAME="opsera-tf-state-${AWS_ACCOUNT_ID}-${REGION}"
          TABLE_NAME="terraform-state-lock-${REGION}"

          # Learning #117: Pass outputs to subsequent steps
          echo "tf_bucket=$BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "tf_table=$TABLE_NAME" >> $GITHUB_OUTPUT

          # Create S3 bucket if not exists
          if ! aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "Creating S3 bucket: $BUCKET_NAME in $REGION"
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region $REGION \
              --create-bucket-configuration LocationConstraint=$REGION

            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled

            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration \
              '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          else
            echo "S3 bucket exists: $BUCKET_NAME"
          fi

          # Create DynamoDB table if not exists
          if ! aws dynamodb describe-table --table-name "$TABLE_NAME" --region $REGION 2>/dev/null; then
            echo "Creating DynamoDB table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION

            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region $REGION
          else
            echo "DynamoDB table exists: $TABLE_NAME"
          fi

      - name: Create ECR Repositories
        if: steps.discover.outputs.ecr_exists == 'false'
        run: |
          ECR_REPO="${{ steps.discover.outputs.ecr_repo }}"
          ECR_REPO_LOWER=$(echo "$ECR_REPO" | tr '[:upper:]' '[:lower:]')

          aws ecr create-repository \
            --repository-name "$ECR_REPO_LOWER-backend" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

          aws ecr create-repository \
            --repository-name "$ECR_REPO_LOWER-frontend" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

      # Learning #112: ACTUAL VPC creation
      - name: Create VPC with Terraform
        if: steps.discover.outputs.vpc_exists == 'false'
        run: |
          echo "Creating VPC: ${{ env.VPC_NAME }}"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)

          mkdir -p /tmp/terraform-vpc
          cd /tmp/terraform-vpc

          cat > main.tf << 'EOF'
          terraform {
            required_version = ">= 1.0.0"
            required_providers {
              aws = { source = "hashicorp/aws", version = "~> 5.0" }
            }
            backend "s3" {}
          }

          provider "aws" { region = var.region }

          variable "vpc_name" {
            type = string
          }
          variable "region" {
            type = string
          }
          variable "vpc_cidr" {
            type    = string
            default = "10.0.0.0/16"
          }
          variable "argocd_cluster" {
            type = string
          }
          variable "workload_cluster" {
            type = string
          }

          data "aws_availability_zones" "available" { state = "available" }

          resource "aws_vpc" "main" {
            cidr_block           = var.vpc_cidr
            enable_dns_hostnames = true
            enable_dns_support   = true
            tags = { Name = var.vpc_name }
          }

          resource "aws_internet_gateway" "main" {
            vpc_id = aws_vpc.main.id
            tags = { Name = "${var.vpc_name}-igw" }
          }

          resource "aws_subnet" "public" {
            count                   = 2
            vpc_id                  = aws_vpc.main.id
            cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
            availability_zone       = data.aws_availability_zones.available.names[count.index]
            map_public_ip_on_launch = true
            tags = {
              Name = "${var.vpc_name}-public-${count.index + 1}"
              "kubernetes.io/role/elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_subnet" "private" {
            count             = 2
            vpc_id            = aws_vpc.main.id
            cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index + 10)
            availability_zone = data.aws_availability_zones.available.names[count.index]
            tags = {
              Name = "${var.vpc_name}-private-${count.index + 1}"
              "kubernetes.io/role/internal-elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_eip" "nat" {
            count  = 2
            domain = "vpc"
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
          }

          resource "aws_nat_gateway" "main" {
            count         = 2
            allocation_id = aws_eip.nat[count.index].id
            subnet_id     = aws_subnet.public[count.index].id
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
            depends_on = [aws_internet_gateway.main]
          }

          resource "aws_route_table" "public" {
            vpc_id = aws_vpc.main.id
            route {
              cidr_block = "0.0.0.0/0"
              gateway_id = aws_internet_gateway.main.id
            }
            tags = { Name = "${var.vpc_name}-public-rt" }
          }

          resource "aws_route_table" "private" {
            count  = 2
            vpc_id = aws_vpc.main.id
            route {
              cidr_block     = "0.0.0.0/0"
              nat_gateway_id = aws_nat_gateway.main[count.index].id
            }
            tags = { Name = "${var.vpc_name}-private-rt-${count.index + 1}" }
          }

          resource "aws_route_table_association" "public" {
            count          = 2
            subnet_id      = aws_subnet.public[count.index].id
            route_table_id = aws_route_table.public.id
          }

          resource "aws_route_table_association" "private" {
            count          = 2
            subnet_id      = aws_subnet.private[count.index].id
            route_table_id = aws_route_table.private[count.index].id
          }

          output "vpc_id" { value = aws_vpc.main.id }
          output "public_subnet_ids" { value = aws_subnet.public[*].id }
          output "private_subnet_ids" { value = aws_subnet.private[*].id }
          EOF

          BUCKET_NAME="${{ steps.tf_backend.outputs.tf_bucket }}"
          TABLE_NAME="${{ steps.tf_backend.outputs.tf_table }}"

          terraform init \
            -backend-config="bucket=${BUCKET_NAME}" \
            -backend-config="key=${{ inputs.app_name }}/vpc/terraform.tfstate" \
            -backend-config="region=${{ inputs.app_region }}" \
            -backend-config="dynamodb_table=${TABLE_NAME}" \
            -backend-config="encrypt=true"

          terraform apply -auto-approve \
            -var="vpc_name=${{ env.VPC_NAME }}" \
            -var="region=${{ inputs.app_region }}" \
            -var="argocd_cluster=${{ env.ARGOCD_CLUSTER }}" \
            -var="workload_cluster=${{ env.WORKLOAD_CLUSTER }}"

          VPC_ID=$(terraform output -raw vpc_id)
          PUBLIC_SUBNETS=$(terraform output -json public_subnet_ids | jq -r '.[]' | tr '\n' ' ')
          PRIVATE_SUBNETS=$(terraform output -json private_subnet_ids | jq -r '.[]' | tr '\n' ' ')

          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "public_subnet_ids=$PUBLIC_SUBNETS" >> $GITHUB_OUTPUT
          echo "private_subnet_ids=$PRIVATE_SUBNETS" >> $GITHUB_OUTPUT

          echo "VPC created successfully: $VPC_ID"

      # Learning #112: ACTUAL EKS cluster creation
      - name: Create ArgoCD EKS Cluster
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          echo "Creating ArgoCD EKS Cluster: ${{ env.ARGOCD_CLUSTER }}"

          VPC_ID="${{ steps.discover.outputs.vpc_id }}"
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
              --query 'Vpcs[0].VpcId' --output text)
          fi

          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          # Create cluster role
          CLUSTER_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          # Create EKS cluster
          aws eks create-cluster \
            --name "${{ env.ARGOCD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}

          # Learning #124: Create OIDC provider for IRSA
          echo "Creating OIDC provider..."
          aws eks describe-cluster --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }} --query 'cluster.identity.oidc.issuer' --output text | \
            sed 's|https://||' | \
            xargs -I {} aws iam list-open-id-connect-providers --query "OpenIDConnectProviderList[?contains(Arn, '{}')]" --output text || \
            aws eks describe-cluster --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }} --query 'cluster.identity.oidc.issuer' --output text | \
            sed 's|https://||' | \
            xargs -I {} aws iam create-open-id-connect-provider \
              --url https://{} \
              --client-id-list sts.amazonaws.com \
              --thumbprint-list $(openssl s_client -servername {} -showcerts -connect {}:443 </dev/null 2>/dev/null | openssl x509 -fingerprint -noout -sha1 | cut -d'=' -f2 | tr '[:upper:]' '[:lower:]') || true

      - name: Create ArgoCD Node Group
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          VPC_ID="${{ steps.discover.outputs.vpc_id }}"
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
              --query 'Vpcs[0].VpcId' --output text)
          fi

          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          # Learning #126: AMI type required for EKS 1.28+
          aws eks create-nodegroup \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --node-role "$NODE_ROLE_ARN" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --ami-type AL2_x86_64 \
            --scaling-config minSize=1,maxSize=3,desiredSize=2 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --region ${{ inputs.app_region }} || true

      - name: Create Workload EKS Cluster
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          echo "Creating Workload EKS Cluster: ${{ env.WORKLOAD_CLUSTER }}"

          VPC_ID="${{ steps.discover.outputs.vpc_id }}"
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
              --query 'Vpcs[0].VpcId' --output text)
          fi

          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          CLUSTER_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-cluster \
            --name "${{ env.WORKLOAD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }}

          # Learning #124: Create OIDC provider for IRSA
          echo "Creating OIDC provider..."
          aws eks describe-cluster --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }} --query 'cluster.identity.oidc.issuer' --output text | \
            sed 's|https://||' | \
            xargs -I {} aws iam list-open-id-connect-providers --query "OpenIDConnectProviderList[?contains(Arn, '{}')]" --output text || \
            aws eks describe-cluster --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }} --query 'cluster.identity.oidc.issuer' --output text | \
            sed 's|https://||' | \
            xargs -I {} aws iam create-open-id-connect-provider \
              --url https://{} \
              --client-id-list sts.amazonaws.com \
              --thumbprint-list $(openssl s_client -servername {} -showcerts -connect {}:443 </dev/null 2>/dev/null | openssl x509 -fingerprint -noout -sha1 | cut -d'=' -f2 | tr '[:upper:]' '[:lower:]') || true

      - name: Create Workload Node Group
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          VPC_ID="${{ steps.discover.outputs.vpc_id }}"
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
              --query 'Vpcs[0].VpcId' --output text)
          fi

          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

            # ELB permissions for LoadBalancer services
            aws iam put-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-name "elb-permissions" \
              --policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Action": ["elasticloadbalancing:*", "ec2:Describe*", "ec2:CreateSecurityGroup",
                    "ec2:AuthorizeSecurityGroupIngress", "ec2:CreateTags"],
                  "Resource": "*"
                }]
              }'
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          # Learning #126: AMI type required for EKS 1.28+
          aws eks create-nodegroup \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --node-role "$NODE_ROLE_ARN" \
            --nodegroup-name "workload-nodes" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --ami-type AL2_x86_64 \
            --scaling-config minSize=1,maxSize=4,desiredSize=2 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --nodegroup-name "workload-nodes" \
            --region ${{ inputs.app_region }} || true

      # Learning #128: Install ExternalDNS for greenfield deployments
      - name: Install ExternalDNS
        if: steps.discover.outputs.deployment_type == 'greenfield'
        run: |
          echo "Installing ExternalDNS for automatic DNS management..."

          # Install kubectl
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

          aws eks update-kubeconfig --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }}

          # Create IAM policy for ExternalDNS Route53 access
          POLICY_NAME="${{ env.WORKLOAD_CLUSTER }}-external-dns-policy"
          if ! aws iam get-policy --policy-arn "arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):policy/$POLICY_NAME" 2>/dev/null; then
            cat > /tmp/externaldns-policy.json << 'EOF'
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": ["route53:ChangeResourceRecordSets"],
                "Resource": ["arn:aws:route53:::hostedzone/*"]
              },
              {
                "Effect": "Allow",
                "Action": [
                  "route53:ListHostedZones",
                  "route53:ListResourceRecordSets"
                ],
                "Resource": ["*"]
              }
            ]
          }
          EOF

            aws iam create-policy \
              --policy-name "$POLICY_NAME" \
              --policy-document file:///tmp/externaldns-policy.json \
              --region ${{ inputs.app_region }} || true
          fi

          # Get OIDC issuer URL
          OIDC_ISSUER=$(aws eks describe-cluster --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }} \
            --query 'cluster.identity.oidc.issuer' --output text | sed 's|https://||')

          # Create IAM role for ExternalDNS with IRSA
          ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-external-dns"
          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          
          if ! aws iam get-role --role-name "$ROLE_NAME" 2>/dev/null; then
            cat > /tmp/externaldns-trust.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ISSUER}"
                },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "${OIDC_ISSUER}:sub": "system:serviceaccount:kube-system:external-dns",
                    "${OIDC_ISSUER}:aud": "sts.amazonaws.com"
                  }
                }
              }
            ]
          }
          EOF

            aws iam create-role \
              --role-name "$ROLE_NAME" \
              --assume-role-policy-document file:///tmp/externaldns-trust.json \
              --region ${{ inputs.app_region }} || true

            aws iam attach-role-policy \
              --role-name "$ROLE_NAME" \
              --policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/$POLICY_NAME" \
              --region ${{ inputs.app_region }} || true
          fi

          ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text)

          # Create service account with IRSA annotation
          kubectl create namespace kube-system || true
          kubectl create serviceaccount external-dns -n kube-system || true
          kubectl annotate serviceaccount external-dns \
            -n kube-system \
            eks.amazonaws.com/role-arn="$ROLE_ARN" || true

          # Install ExternalDNS via Helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/ || true
          helm repo update

          helm upgrade --install external-dns external-dns/external-dns \
            --namespace kube-system \
            --set provider=aws \
            --set aws.region=${{ inputs.app_region }} \
            --set txtOwnerId=${{ env.WORKLOAD_CLUSTER }} \
            --set policy=sync \
            --set serviceAccount.create=false \
            --set serviceAccount.name=external-dns \
            --wait || true

          echo "✅ ExternalDNS installed successfully"

      - name: Install ArgoCD
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          echo "Installing ArgoCD on cluster: ${{ env.ARGOCD_CLUSTER }}"
          
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}

          kubectl create namespace argocd || true
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

          echo "Waiting for ArgoCD server to be available..."
          kubectl wait --for=condition=available --timeout=600s deployment/argocd-server -n argocd || true
          
          # Learning #162: Create Repository Secret for private repo access
          echo "Creating ArgoCD Repository Secret..."
          kubectl create secret generic repo-${{ inputs.app_name }} \
            --namespace argocd \
            --from-literal=type=git \
            --from-literal=url=https://github.com/${{ github.repository }}.git \
            --from-literal=username=git \
            --from-literal=password=${{ secrets.GITHUB_TOKEN }} \
            --dry-run=client -o yaml | \
            kubectl label -f - argocd.argoproj.io/secret-type=repository --local -o yaml | \
            kubectl apply -f - || echo "Repository secret may already exist"
          
          echo "✅ ArgoCD installed and configured"

  application:
    name: "Phase 2: Application"
    runs-on: ubuntu-latest
    needs: [infrastructure]
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: master
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Create or Checkout Deploy Branch (Learning #160)
        run: |
          echo "========================================"
          echo "SETTING UP DEPLOY BRANCH"
          echo "========================================"
          
          DEPLOY_BRANCH="${{ env.DEPLOY_BRANCH }}"
          
          # Learning #160: Deploy branch for artifacts, main only for GHA workflows
          # Check if branch exists
          if git ls-remote --heads origin "$DEPLOY_BRANCH" | grep -q "$DEPLOY_BRANCH"; then
            echo "Branch $DEPLOY_BRANCH exists, checking out..."
            git checkout "$DEPLOY_BRANCH"
            git pull origin "$DEPLOY_BRANCH" || true
          else
            echo "Branch $DEPLOY_BRANCH does not exist, creating from main/master..."
            # Try main first, fallback to master
            if git show-ref --verify --quiet refs/heads/main; then
              git checkout main
            elif git show-ref --verify --quiet refs/heads/master; then
              git checkout master
            else
              echo "ERROR: Neither main nor master branch found"
              exit 1
            fi
            git checkout -b "$DEPLOY_BRANCH"
            git push -u origin "$DEPLOY_BRANCH" || echo "Failed to push branch, will continue"
          fi
          
          echo "✅ Using deploy branch: $DEPLOY_BRANCH"

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Build & Push Images
        id: build
        run: |
          AWS_ACCOUNT_ID=${{ needs.infrastructure.outputs.aws_account_id }}
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ inputs.app_region }}.amazonaws.com"
          ECR_REPO_LOWER=$(echo "${{ needs.infrastructure.outputs.ecr_repo }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_TAG="${{ github.sha }}"

          aws ecr get-login-password --region ${{ inputs.app_region }} | \
            docker login --username AWS --password-stdin $ECR_REGISTRY

          # Build and push backend
          if [ -f "backend/Dockerfile" ]; then
            docker build -t $ECR_REGISTRY/$ECR_REPO_LOWER-backend:$IMAGE_TAG ./backend
            docker push $ECR_REGISTRY/$ECR_REPO_LOWER-backend:$IMAGE_TAG
            
            # Learning #134: Tag and push with 'latest' as well
            docker tag $ECR_REGISTRY/$ECR_REPO_LOWER-backend:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPO_LOWER-backend:latest
            docker push $ECR_REGISTRY/$ECR_REPO_LOWER-backend:latest
            echo "Backend image pushed: $ECR_REGISTRY/$ECR_REPO_LOWER-backend:$IMAGE_TAG and :latest"
          fi

          # Build and push frontend
          if [ -f "frontend/Dockerfile" ]; then
            docker build -t $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:$IMAGE_TAG ./frontend
            docker push $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:$IMAGE_TAG
            
            # Learning #134: Tag and push with 'latest' as well
            docker tag $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:latest
            docker push $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:latest
            echo "Frontend image pushed: $ECR_REGISTRY/$ECR_REPO_LOWER-frontend:$IMAGE_TAG and :latest"
          fi

          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT
          echo "ecr_repo_backend=$ECR_REPO_LOWER-backend" >> $GITHUB_OUTPUT
          echo "ecr_repo_frontend=$ECR_REPO_LOWER-frontend" >> $GITHUB_OUTPUT

      - name: Update Kustomization
        run: |
          cd ${{ inputs.app_name }}-opsera/k8s/overlays/${{ inputs.app_env }}

          AWS_ACCOUNT_ID=${{ needs.infrastructure.outputs.aws_account_id }}
          ECR_REGISTRY="${{ steps.build.outputs.ecr_registry }}"
          IMAGE_TAG="${{ steps.build.outputs.image_tag }}"
          ECR_BACKEND="${{ steps.build.outputs.ecr_registry }}/${{ steps.build.outputs.ecr_repo_backend }}"
          ECR_FRONTEND="${{ steps.build.outputs.ecr_registry }}/${{ steps.build.outputs.ecr_repo_frontend }}"

          echo "Updating kustomization.yaml with:"
          echo "  Backend: $ECR_BACKEND:$IMAGE_TAG"
          echo "  Frontend: $ECR_FRONTEND:$IMAGE_TAG"

          # Replace backend image
          sed -i "s|PLACEHOLDER_ECR_BACKEND|${ECR_BACKEND}|g" kustomization.yaml
          sed -i "/name: PLACEHOLDER_ECR_BACKEND/,/newTag:/ s|newName: .*|newName: ${ECR_BACKEND}|" kustomization.yaml
          sed -i "/name: PLACEHOLDER_ECR_BACKEND/,/newTag:/ s|newTag: .*|newTag: \"${IMAGE_TAG}\"|" kustomization.yaml

          # Replace frontend image
          sed -i "s|PLACEHOLDER_ECR_FRONTEND|${ECR_FRONTEND}|g" kustomization.yaml
          sed -i "/name: PLACEHOLDER_ECR_FRONTEND/,/newTag:/ s|newName: .*|newName: ${ECR_FRONTEND}|" kustomization.yaml
          sed -i "/name: PLACEHOLDER_ECR_FRONTEND/,/newTag:/ s|newTag: .*|newTag: \"${IMAGE_TAG}\"|" kustomization.yaml

          # Update all tags to use the same tag
          sed -i "s|newTag: .*|newTag: \"${IMAGE_TAG}\"|g" kustomization.yaml

          # Validate no placeholders
          if grep -q "PLACEHOLDER\|TODO\|CHANGEME" kustomization.yaml; then
            echo "ERROR: Placeholders found!"
            grep -n "PLACEHOLDER\|TODO\|CHANGEME" kustomization.yaml
            exit 1
          fi

          echo ""
          echo "=== Updated kustomization.yaml ==="
          cat kustomization.yaml

      - name: Commit Changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add ${{ inputs.app_name }}-opsera/k8s/overlays/${{ inputs.app_env }}/kustomization.yaml
          git commit -m "[skip ci] Update image tags to ${{ steps.build.outputs.image_tag }}" || echo "No changes"
          git push origin ${{ env.DEPLOY_BRANCH }} || echo "Push failed"

  verification:
    name: "Phase 3: Verification"
    runs-on: ubuntu-latest
    needs: [infrastructure, application]
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: master
          fetch-depth: 0

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.workload_cluster }}" --region ${{ inputs.app_region }}

      - name: Create AWS Credentials Secret
        run: |
          # Learning #139: Create aws-credentials secret for IRSA fallback
          kubectl delete secret aws-credentials -n ${{ needs.infrastructure.outputs.namespace }} --ignore-not-found || true
          kubectl create secret generic aws-credentials \
            --namespace ${{ needs.infrastructure.outputs.namespace }} \
            --from-literal=AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} \
            --from-literal=AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }} || true

      - name: Register Workload Cluster with ArgoCD (Learning #158)
        run: |
          echo "========================================"
          echo "REGISTERING WORKLOAD CLUSTER WITH ARGOCD"
          echo "========================================"
          
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.argocd_cluster }}" --region ${{ inputs.app_region }}
          
          # Learning #158: Create ServiceAccount in workload cluster for ArgoCD access
          WORKLOAD_CLUSTER="${{ needs.infrastructure.outputs.workload_cluster }}"
          
          echo "Step 1: Creating argocd-manager ServiceAccount in workload cluster..."
          aws eks update-kubeconfig --name "$WORKLOAD_CLUSTER" --region ${{ inputs.app_region }}
          
          kubectl create namespace argocd-manager || true
          
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: argocd-manager
            namespace: argocd-manager
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: argocd-manager-role-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
            - kind: ServiceAccount
              name: argocd-manager
              namespace: argocd-manager
          EOF
          
          # Get ServiceAccount token
          echo "Step 2: Getting ServiceAccount token..."
          sleep 5  # Wait for token to be created
          SA_TOKEN=$(kubectl get secret -n argocd-manager -o jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="argocd-manager")].data.token}' | base64 -d || echo "")
          
          if [ -z "$SA_TOKEN" ]; then
            echo "⚠️  ServiceAccount token not found, creating manually..."
            # Create token secret manually
            kubectl apply -f - <<EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: argocd-manager-token
            namespace: argocd-manager
            annotations:
              kubernetes.io/service-account.name: argocd-manager
          type: kubernetes.io/service-account-token
          EOF
            sleep 5
            SA_TOKEN=$(kubectl get secret argocd-manager-token -n argocd-manager -o jsonpath='{.data.token}' | base64 -d)
          fi
          
          # Get cluster CA cert
          WORKLOAD_CA=$(aws eks describe-cluster --name "$WORKLOAD_CLUSTER" --region ${{ inputs.app_region }} --query 'cluster.certificateAuthority.data' --output text)
          WORKLOAD_ENDPOINT=$(aws eks describe-cluster --name "$WORKLOAD_CLUSTER" --region ${{ inputs.app_region }} --query 'cluster.endpoint' --output text)
          
          echo "Step 3: Creating Cluster Secret in ArgoCD cluster..."
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.argocd_cluster }}" --region ${{ inputs.app_region }}
          
          # Learning #158: Create Cluster Secret with bearer token
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: ${WORKLOAD_CLUSTER}-secret
            namespace: argocd
            labels:
              argocd.argoproj.io/secret-type: cluster
          type: Opaque
          stringData:
            name: ${WORKLOAD_CLUSTER}
            server: ${WORKLOAD_ENDPOINT}
            config: |
              {
                "bearerToken": "${SA_TOKEN}",
                "tlsClientConfig": {
                  "caData": "${WORKLOAD_CA}",
                  "insecure": false
                }
              }
          EOF
          
          echo "✅ Workload cluster registered with ArgoCD"
          echo "   Cluster: $WORKLOAD_CLUSTER"
          echo "   Endpoint: $WORKLOAD_ENDPOINT"

      - name: Apply ArgoCD Application (Learning #164)
        run: |
          echo "========================================"
          echo "APPLYING ARGOCD APPLICATION"
          echo "========================================"
          
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.argocd_cluster }}" --region ${{ inputs.app_region }}
          
          # Learning #164: ArgoCD Application must be APPLIED to cluster, not just in Git
          WORKLOAD_NAME="${{ needs.infrastructure.outputs.workload_cluster }}"
          WORKLOAD_ENDPOINT=$(aws eks describe-cluster --name "$WORKLOAD_NAME" --region ${{ inputs.app_region }} --query 'cluster.endpoint' --output text)
          APP_NAME="${{ inputs.app_name }}-${{ inputs.app_env }}"
          
          # Learning #160: Use deploy branch for manifests
          DEPLOY_BRANCH="${{ env.DEPLOY_BRANCH }}"
          
          # Create ArgoCD Application manifest
          cat <<EOF | kubectl apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: ${APP_NAME}
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}
              targetRevision: ${DEPLOY_BRANCH}
              path: ${{ inputs.app_name }}-opsera/k8s/overlays/${{ inputs.app_env }}
            destination:
              server: ${WORKLOAD_ENDPOINT}
              namespace: ${{ needs.infrastructure.outputs.namespace }}
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
                allowEmpty: false
              syncOptions:
                - CreateNamespace=true
                - PrunePropagationPolicy=foreground
                - PruneLast=true
            revisionHistoryLimit: 3
          EOF
          
          echo "✅ ArgoCD Application created: $APP_NAME"
          echo "   Target cluster: $WORKLOAD_NAME"
          echo "   Source branch: $DEPLOY_BRANCH"

      - name: Wait for ArgoCD Sync
        run: |
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.argocd_cluster }}" --region ${{ inputs.app_region }}
          APP_NAME="${{ inputs.app_name }}-${{ inputs.app_env }}"
          for i in {1..60}; do
            STATUS=$(kubectl get application "$APP_NAME" -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
            if [ "$STATUS" = "Synced" ]; then
              echo "✅ ArgoCD sync complete"
              break
            fi
            echo "Waiting for ArgoCD sync... ($i/60) - Status: $STATUS"
            sleep 10
          done

      - name: Verify Deployment
        run: |
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.workload_cluster }}" --region ${{ inputs.app_region }}
          
          NAMESPACE="${{ needs.infrastructure.outputs.namespace }}"
          APP_NAME="${{ inputs.app_name }}"
          
          # Wait for namespace to be created by ArgoCD
          echo "Waiting for namespace $NAMESPACE to be created..."
          for i in {1..30}; do
            if kubectl get namespace "$NAMESPACE" 2>/dev/null; then
              echo "✅ Namespace $NAMESPACE exists"
              break
            fi
            echo "Waiting for namespace... ($i/30)"
            sleep 10
          done
          
          # Check if namespace exists, if not, exit with error
          if ! kubectl get namespace "$NAMESPACE" 2>/dev/null; then
            echo "❌ Namespace $NAMESPACE not found after waiting"
            echo "Checking ArgoCD application status..."
            aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.argocd_cluster }}" --region ${{ inputs.app_region }}
            kubectl get application "${{ inputs.app_name }}-${{ inputs.app_env }}" -n argocd -o yaml || true
            exit 1
          fi
          
          # Wait for pods
          echo "Waiting for backend pods..."
          kubectl wait --for=condition=ready pod -l app=${APP_NAME}-backend -n $NAMESPACE --timeout=300s || true
          echo "Waiting for frontend pods..."
          kubectl wait --for=condition=ready pod -l app=${APP_NAME}-frontend -n $NAMESPACE --timeout=300s || true

          # Get LoadBalancer endpoint
          for i in {1..30}; do
            ENDPOINT=$(kubectl get svc ${APP_NAME}-frontend -n $NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [ -n "$ENDPOINT" ]; then
              echo "endpoint=$ENDPOINT" >> $GITHUB_OUTPUT
              echo "✅ LoadBalancer endpoint: $ENDPOINT"
              break
            fi
            sleep 10
          done

      - name: Verify HTTP 200
        run: |
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.workload_cluster }}" --region ${{ inputs.app_region }}
          
          NAMESPACE="${{ needs.infrastructure.outputs.namespace }}"
          APP_NAME="${{ inputs.app_name }}"
          
          # Check if namespace exists first
          if ! kubectl get namespace "$NAMESPACE" 2>/dev/null; then
            echo "❌ Namespace $NAMESPACE not found"
            exit 1
          fi
          
          ENDPOINT=$(kubectl get svc ${APP_NAME}-frontend -n $NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          
          if [ -z "$ENDPOINT" ]; then
            echo "❌ LoadBalancer endpoint not found"
            echo "Checking service status..."
            kubectl get svc -n $NAMESPACE || true
            exit 1
          fi
          
          echo "Testing endpoint: http://$ENDPOINT"
          
          for i in {1..30}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "http://$ENDPOINT" || echo "000")
            echo "Attempt $i/30: HTTP $STATUS"
            if [ "$STATUS" = "200" ]; then
              echo "✅ ENDPOINT VERIFIED: http://$ENDPOINT"
              exit 0
            fi
            sleep 10
          done
          
          echo "❌ Endpoint not responding with HTTP 200"
          exit 1

      - name: Summary
        run: |
          echo "## Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ArgoCD Cluster | ${{ needs.infrastructure.outputs.argocd_cluster }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workload Cluster | ${{ needs.infrastructure.outputs.workload_cluster }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deployment Type | ${{ needs.infrastructure.outputs.deployment_type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Namespace | ${{ needs.infrastructure.outputs.namespace }} |" >> $GITHUB_STEP_SUMMARY
