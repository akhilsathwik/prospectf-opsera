name: Deploy to AWS EKS (Unified)

on:
  workflow_dispatch:
    inputs:
      tenant_name:
        description: 'Tenant/Organization name'
        required: true
        type: string
        default: 'opsera-se'
      app_name:
        description: 'Application name'
        required: true
        type: string
        default: 'prospectf500-app1'
      app_env:
        description: 'Target environment'
        required: true
        type: choice
        options: [dev, staging, prod]
        default: 'dev'
      app_region:
        description: 'AWS deployment region'
        required: true
        type: string
        default: 'eu-north-1'

permissions:
  contents: write
  id-token: write
  actions: read

# NOTE: Short naming convention (Learning #118)
# - Region short: eu-north-1 -> eun1
# - Tenant short: first 6 chars, lowercase
# - Cluster env short: nonprod -> np, prod -> prod

env:
  DEPLOY_BRANCH: ${{ inputs.app_name }}-deploy
  # Short region code (Learning #118)
  REGION_SHORT: ${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'eu-north-1' && 'eun1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}
  # Short cluster env (np = nonprod)
  CLUSTER_ENV_SHORT: ${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  # Resource names using short convention
  VPC_NAME: opsera-vpc  # Hardcoded for all deployments
  ARGOCD_CLUSTER: argocd-${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'eu-north-1' && 'eun1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}
  WORKLOAD_CLUSTER: ${{ inputs.tenant_name }}-${{ inputs.app_region == 'us-west-2' && 'usw2' || inputs.app_region == 'us-west-1' && 'usw1' || inputs.app_region == 'us-east-1' && 'use1' || inputs.app_region == 'us-east-2' && 'use2' || inputs.app_region == 'eu-west-1' && 'euw1' || inputs.app_region == 'eu-west-2' && 'euw2' || inputs.app_region == 'eu-central-1' && 'euc1' || inputs.app_region == 'eu-north-1' && 'eun1' || inputs.app_region == 'ap-south-1' && 'aps1' || inputs.app_region == 'ap-southeast-1' && 'apse1' || inputs.app_region == 'ap-northeast-1' && 'apne1' || inputs.app_region }}-${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  ECR_REPO: ${{ inputs.tenant_name }}/${{ inputs.app_name }}
  NAMESPACE: ${{ inputs.app_name }}-${{ inputs.app_env }}

jobs:
  infrastructure:
    name: "Phase 1: Infrastructure"
    runs-on: ubuntu-latest
    outputs:
      argocd_cluster: ${{ steps.discover.outputs.argocd_cluster }}
      workload_cluster: ${{ steps.discover.outputs.workload_cluster }}
      ecr_repo: ${{ steps.discover.outputs.ecr_repo }}
      namespace: ${{ steps.discover.outputs.namespace }}
      deployment_type: ${{ steps.discover.outputs.deployment_type }}
      aws_account_id: ${{ steps.discover.outputs.aws_account_id }}
      vpc_id: ${{ steps.discover.outputs.vpc_id }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Auto-Update ArgoCD Application Repository URL
        run: |
          echo "========================================"
          echo "AUTO-UPDATING ARGOCD APPLICATION"
          echo "========================================"
          
          ARGOCD_APP="argocd-unified/application.yaml"
          
          if [ ! -f "$ARGOCD_APP" ]; then
            echo "⚠️  ArgoCD application file not found: $ARGOCD_APP"
            exit 0
          fi
          
          # Get repository URL from GitHub context
          # Format: https://github.com/owner/repo.git
          REPO_URL="${{ github.server_url }}/${{ github.repository }}.git"
          
          echo "Detected repository URL: $REPO_URL"
          echo "Updating $ARGOCD_APP..."
          
          # Update repoURL in application.yaml
          sed -i "s|repoURL:.*|repoURL: $REPO_URL|" "$ARGOCD_APP"
          
          echo "✅ Updated ArgoCD application manifest"
          echo ""
          echo "Verification:"
          grep "repoURL:" "$ARGOCD_APP"
          
          # Commit the change if it was modified
          if git diff --quiet "$ARGOCD_APP"; then
            echo "No changes needed (already correct)"
          else
            echo "Changes detected, committing update..."
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add "$ARGOCD_APP"
            git commit -m "[skip ci] Auto-update ArgoCD application repository URL" || echo "No changes to commit"
            git push || echo "Push skipped (may already be up to date)"
          fi

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Discover Infrastructure
        id: discover
        run: |
          echo "========================================"
          echo "INFRASTRUCTURE DISCOVERY"
          echo "========================================"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT

          ARGOCD_CLUSTER="${{ env.ARGOCD_CLUSTER }}"
          WORKLOAD_CLUSTER="${{ env.WORKLOAD_CLUSTER }}"
          ECR_REPO="${{ env.ECR_REPO }}"
          NAMESPACE="${{ env.NAMESPACE }}"

          echo "argocd_cluster=$ARGOCD_CLUSTER" >> $GITHUB_OUTPUT
          echo "workload_cluster=$WORKLOAD_CLUSTER" >> $GITHUB_OUTPUT
          echo "ecr_repo=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "namespace=$NAMESPACE" >> $GITHUB_OUTPUT

          # Check VPC (Learning #116)
          echo "Checking VPC: ${{ env.VPC_NAME }}..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ -n "$VPC_ID" ] && [ "$VPC_ID" != "null" ]; then
            VPC_EXISTS="true"
            echo "  VPC EXISTS: $VPC_ID"
          else
            VPC_EXISTS="false"
            echo "  VPC NOT FOUND - will create"
          fi
          echo "vpc_exists=$VPC_EXISTS" >> $GITHUB_OUTPUT
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

          # Check ArgoCD Cluster
          echo "Checking ArgoCD Cluster: $ARGOCD_CLUSTER..."
          if aws eks describe-cluster --name "$ARGOCD_CLUSTER" 2>/dev/null; then
            ARGOCD_EXISTS="true"
            echo "  ArgoCD cluster EXISTS"
          else
            ARGOCD_EXISTS="false"
            echo "  ArgoCD cluster NOT FOUND - will create"
          fi
          echo "argocd_exists=$ARGOCD_EXISTS" >> $GITHUB_OUTPUT

          # Check Workload Cluster
          echo "Checking Workload Cluster: $WORKLOAD_CLUSTER..."
          if aws eks describe-cluster --name "$WORKLOAD_CLUSTER" 2>/dev/null; then
            WORKLOAD_EXISTS="true"
            echo "  Workload cluster EXISTS"
          else
            WORKLOAD_EXISTS="false"
            echo "  Workload cluster NOT FOUND - will create"
          fi
          echo "workload_exists=$WORKLOAD_EXISTS" >> $GITHUB_OUTPUT

          # Check ECR Repository
          echo "Checking ECR Repository: $ECR_REPO..."
          ECR_REPO_LOWER=$(echo "$ECR_REPO" | tr '[:upper:]' '[:lower:]')
          if aws ecr describe-repositories --repository-names "$ECR_REPO_LOWER" 2>/dev/null; then
            ECR_EXISTS="true"
            echo "  ECR repository EXISTS"
          else
            ECR_EXISTS="false"
            echo "  ECR repository NOT FOUND - will create"
          fi
          echo "ecr_exists=$ECR_EXISTS" >> $GITHUB_OUTPUT

          # Determine deployment type
          if [ "$ARGOCD_EXISTS" = "true" ] && [ "$WORKLOAD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="brownfield"
          elif [ "$ARGOCD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="partial"
          else
            DEPLOY_TYPE="greenfield"
          fi
          echo "deployment_type=$DEPLOY_TYPE" >> $GITHUB_OUTPUT

          echo "========================================"
          echo "DEPLOYMENT TYPE: $DEPLOY_TYPE"
          echo "========================================"

      # Learning #114: Region-specific Terraform state bucket
      - name: Setup Terraform State Backend
        id: tf_backend
        run: |
          echo "Setting up Terraform State Backend..."

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          REGION="${{ inputs.app_region }}"

          # CRITICAL: Use region-specific bucket name (Learning #114)
          BUCKET_NAME="opsera-tf-state-${AWS_ACCOUNT_ID}-${REGION}"
          TABLE_NAME="terraform-state-lock-${REGION}"

          # Learning #117: Pass outputs to subsequent steps
          echo "tf_bucket=$BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "tf_table=$TABLE_NAME" >> $GITHUB_OUTPUT

          # Create S3 bucket if not exists
          if ! aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "Creating S3 bucket: $BUCKET_NAME in $REGION"
            if [ "$REGION" = "us-east-1" ]; then
              aws s3api create-bucket \
                --bucket "$BUCKET_NAME" \
                --region $REGION
            else
              aws s3api create-bucket \
                --bucket "$BUCKET_NAME" \
                --region $REGION \
                --create-bucket-configuration LocationConstraint=$REGION
            fi

            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled

            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration \
              '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          else
            echo "S3 bucket exists: $BUCKET_NAME"
          fi

          # Create DynamoDB table if not exists
          if ! aws dynamodb describe-table --table-name "$TABLE_NAME" --region $REGION 2>/dev/null; then
            echo "Creating DynamoDB table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION

            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region $REGION
          else
            echo "DynamoDB table exists: $TABLE_NAME"
          fi

      - name: Create ECR Repositories
        if: steps.discover.outputs.ecr_exists == 'false'
        run: |
          ECR_REPO="${{ steps.discover.outputs.ecr_repo }}"
          ECR_REPO_LOWER=$(echo "$ECR_REPO" | tr '[:upper:]' '[:lower:]')

          aws ecr create-repository \
            --repository-name "$ECR_REPO_LOWER" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

          aws ecr create-repository \
            --repository-name "${ECR_REPO_LOWER}-backend" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

          aws ecr create-repository \
            --repository-name "${ECR_REPO_LOWER}-frontend" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

      # Learning #112: ACTUAL VPC creation (not just discovery)
      - name: Create VPC with Terraform
        if: steps.discover.outputs.vpc_exists == 'false'
        run: |
          echo "Creating VPC: ${{ env.VPC_NAME }}"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)

          mkdir -p /tmp/terraform-vpc
          cd /tmp/terraform-vpc

          cat > main.tf << 'EOF'
          terraform {
            required_version = ">= 1.0.0"
            required_providers {
              aws = { source = "hashicorp/aws", version = "~> 5.0" }
            }
            backend "s3" {}
          }

          provider "aws" { region = var.region }

          variable "vpc_name" { type = string }
          variable "region" { type = string }
          variable "vpc_cidr" { type = string, default = "10.0.0.0/16" }
          variable "argocd_cluster" { type = string }
          variable "workload_cluster" { type = string }

          data "aws_availability_zones" "available" { state = "available" }

          resource "aws_vpc" "main" {
            cidr_block           = var.vpc_cidr
            enable_dns_hostnames = true
            enable_dns_support   = true
            tags = { Name = var.vpc_name }
          }

          resource "aws_internet_gateway" "main" {
            vpc_id = aws_vpc.main.id
            tags = { Name = "${var.vpc_name}-igw" }
          }

          resource "aws_subnet" "public" {
            count                   = 2
            vpc_id                  = aws_vpc.main.id
            cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
            availability_zone       = data.aws_availability_zones.available.names[count.index]
            map_public_ip_on_launch = true
            tags = {
              Name = "${var.vpc_name}-public-${count.index + 1}"
              "kubernetes.io/role/elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_subnet" "private" {
            count             = 2
            vpc_id            = aws_vpc.main.id
            cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index + 10)
            availability_zone = data.aws_availability_zones.available.names[count.index]
            tags = {
              Name = "${var.vpc_name}-private-${count.index + 1}"
              "kubernetes.io/role/internal-elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_eip" "nat" {
            count  = 2
            domain = "vpc"
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
          }

          resource "aws_nat_gateway" "main" {
            count         = 2
            allocation_id = aws_eip.nat[count.index].id
            subnet_id     = aws_subnet.public[count.index].id
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
            depends_on = [aws_internet_gateway.main]
          }

          resource "aws_route_table" "public" {
            vpc_id = aws_vpc.main.id
            route { cidr_block = "0.0.0.0/0", gateway_id = aws_internet_gateway.main.id }
            tags = { Name = "${var.vpc_name}-public-rt" }
          }

          resource "aws_route_table" "private" {
            count  = 2
            vpc_id = aws_vpc.main.id
            route { cidr_block = "0.0.0.0/0", nat_gateway_id = aws_nat_gateway.main[count.index].id }
            tags = { Name = "${var.vpc_name}-private-rt-${count.index + 1}" }
          }

          resource "aws_route_table_association" "public" {
            count          = 2
            subnet_id      = aws_subnet.public[count.index].id
            route_table_id = aws_route_table.public.id
          }

          resource "aws_route_table_association" "private" {
            count          = 2
            subnet_id      = aws_subnet.private[count.index].id
            route_table_id = aws_route_table.private[count.index].id
          }

          output "vpc_id" { value = aws_vpc.main.id }
          output "public_subnet_ids" { value = aws_subnet.public[*].id }
          output "private_subnet_ids" { value = aws_subnet.private[*].id }
          EOF

          # Learning #117: Use step outputs for bucket/table names
          BUCKET_NAME="${{ steps.tf_backend.outputs.tf_bucket }}"
          TABLE_NAME="${{ steps.tf_backend.outputs.tf_table }}"

          terraform init \
            -backend-config="bucket=${BUCKET_NAME}" \
            -backend-config="key=${{ inputs.app_name }}/vpc/terraform.tfstate" \
            -backend-config="region=${{ inputs.app_region }}" \
            -backend-config="dynamodb_table=${TABLE_NAME}" \
            -backend-config="encrypt=true"

          terraform apply -auto-approve \
            -var="vpc_name=${{ env.VPC_NAME }}" \
            -var="region=${{ inputs.app_region }}" \
            -var="argocd_cluster=${{ env.ARGOCD_CLUSTER }}" \
            -var="workload_cluster=${{ env.WORKLOAD_CLUSTER }}"

          echo "VPC created successfully"

      # Learning #112: ACTUAL EKS cluster creation
      - name: Create ArgoCD EKS Cluster
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          echo "Creating ArgoCD EKS Cluster: ${{ env.ARGOCD_CLUSTER }}"

          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          # Create cluster role
          CLUSTER_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          # Create EKS cluster
          aws eks create-cluster \
            --name "${{ env.ARGOCD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}

      - name: Create ArgoCD Node Group
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-nodegroup \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --node-role "$NODE_ROLE_ARN" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --scaling-config minSize=1,maxSize=3,desiredSize=2 \
            --ami-type AL2_x86_64 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --region ${{ inputs.app_region }} || true

      - name: Create Workload EKS Cluster
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          echo "Creating Workload EKS Cluster: ${{ env.WORKLOAD_CLUSTER }}"

          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          CLUSTER_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-cluster \
            --name "${{ env.WORKLOAD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }}

      - name: Create Workload Node Group
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

            # ELB permissions for LoadBalancer services
            aws iam put-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-name "elb-permissions" \
              --policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Action": ["elasticloadbalancing:*", "ec2:Describe*", "ec2:CreateSecurityGroup",
                    "ec2:AuthorizeSecurityGroupIngress", "ec2:CreateTags"],
                  "Resource": "*"
                }]
              }'
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-nodegroup \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --nodegroup-name "workload-nodes" \
            --node-role "$NODE_ROLE_ARN" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --scaling-config minSize=1,maxSize=4,desiredSize=2 \
            --ami-type AL2_x86_64 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --nodegroup-name "workload-nodes" \
            --region ${{ inputs.app_region }} || true

      - name: Install ArgoCD
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}

          kubectl create namespace argocd || true
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

          kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd || true

  application:
    name: "Phase 2: Application"
    runs-on: ubuntu-latest
    needs: [infrastructure]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEPLOY_BRANCH }}

      - name: Auto-Update ArgoCD Application Repository URL
        run: |
          echo "========================================"
          echo "AUTO-UPDATING ARGOCD APPLICATION"
          echo "========================================"
          
          ARGOCD_APP="argocd-unified/application.yaml"
          
          if [ ! -f "$ARGOCD_APP" ]; then
            echo "⚠️  ArgoCD application file not found: $ARGOCD_APP"
            exit 0
          fi
          
          # Get repository URL from GitHub context
          REPO_URL="${{ github.server_url }}/${{ github.repository }}.git"
          
          echo "Detected repository URL: $REPO_URL"
          echo "Updating $ARGOCD_APP..."
          
          # Update repoURL in application.yaml
          sed -i "s|repoURL:.*|repoURL: $REPO_URL|" "$ARGOCD_APP"
          
          echo "✅ Updated ArgoCD application manifest"
          echo ""
          echo "Verification:"
          grep "repoURL:" "$ARGOCD_APP"
          
          # Commit the change if it was modified
          if git diff --quiet "$ARGOCD_APP"; then
            echo "No changes needed (already correct)"
          else
            echo "Changes detected, committing update..."
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add "$ARGOCD_APP"
            git commit -m "[skip ci] Auto-update ArgoCD application repository URL" || echo "No changes to commit"
            git push || echo "Push skipped (may already be up to date)"
          fi

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Build & Push Images
        run: |
          AWS_ACCOUNT_ID=${{ needs.infrastructure.outputs.aws_account_id }}
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ inputs.app_region }}.amazonaws.com"
          ECR_REPO="${{ needs.infrastructure.outputs.ecr_repo }}"
          ECR_REPO_LOWER=$(echo "$ECR_REPO" | tr '[:upper:]' '[:lower:]')
          TAG="${{ github.sha }}"

          aws ecr get-login-password --region ${{ inputs.app_region }} | \
            docker login --username AWS --password-stdin $ECR_REGISTRY

          # Build and push backend
          if [ -f "backend/Dockerfile" ]; then
            docker build -t $ECR_REGISTRY/${ECR_REPO_LOWER}-backend:$TAG ./backend
            docker push $ECR_REGISTRY/${ECR_REPO_LOWER}-backend:$TAG
            # Learning #134: Tag with both SHA and latest
            docker tag $ECR_REGISTRY/${ECR_REPO_LOWER}-backend:$TAG $ECR_REGISTRY/${ECR_REPO_LOWER}-backend:latest
            docker push $ECR_REGISTRY/${ECR_REPO_LOWER}-backend:latest
          fi

          # Build and push frontend
          if [ -f "frontend/Dockerfile" ]; then
            docker build -t $ECR_REGISTRY/${ECR_REPO_LOWER}-frontend:$TAG ./frontend
            docker push $ECR_REGISTRY/${ECR_REPO_LOWER}-frontend:$TAG
            # Learning #134: Tag with both SHA and latest
            docker tag $ECR_REGISTRY/${ECR_REPO_LOWER}-frontend:$TAG $ECR_REGISTRY/${ECR_REPO_LOWER}-frontend:latest
            docker push $ECR_REGISTRY/${ECR_REPO_LOWER}-frontend:latest
          fi

  verification:
    name: "Phase 3: Verification"
    runs-on: ubuntu-latest
    needs: [infrastructure, application]
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Verify Infrastructure
        run: |
          echo "ArgoCD Cluster:"
          aws eks describe-cluster --name "${{ needs.infrastructure.outputs.argocd_cluster }}" \
            --query 'cluster.status' --output text || echo "NOT FOUND"

          echo "Workload Cluster:"
          aws eks describe-cluster --name "${{ needs.infrastructure.outputs.workload_cluster }}" \
            --query 'cluster.status' --output text || echo "NOT FOUND"

      - name: Summary
        run: |
          echo "## Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ArgoCD Cluster | ${{ needs.infrastructure.outputs.argocd_cluster }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workload Cluster | ${{ needs.infrastructure.outputs.workload_cluster }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deployment Type | ${{ needs.infrastructure.outputs.deployment_type }} |" >> $GITHUB_STEP_SUMMARY
